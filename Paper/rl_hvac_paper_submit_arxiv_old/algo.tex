\begin{algorithm}[ht]
\caption{Continual Model-Based RL with Hypernetwork for Environment Modeling}
\label{alg:hypernet-rl}
\begin{algorithmic}[1]

\State Initialize policy $\pi_{\theta}$
\State Initialize hypernetwork $H_{\varphi}$, target network $T_{\delta}$
\State Initialize real data memory $\mathcal{M}_{\alpha}$, synthetic data memory $\mathcal{M}_{\beta}$
\State Initialize hypernet train memory $\mathcal{M}_{\gamma}$
\State Initialize regularization constant $\alpha$

\For{each task $T_i$ in sequence}
    \State Reinitialize policy $\pi_{\theta}$
    \State Set training iterations $K_i$
    \For{$k = 1$ to $K_i$}
        \State Interact with real environment $T_i$: collect $(s, a, r, s')$ using $\pi_{\theta}$ and store in $\mathcal{M}_{\alpha}, \mathcal{M}_{\gamma}$
        \State Train $H_{\varphi}$ using samples from $\mathcal{M}_{\gamma}$:
        \State \quad $T_{\delta} \gets H_{\varphi}(\texttt{task\_id}, \texttt{layer\_id})$
        \State \quad sample $(s,a,s',r)$ from $\mathcal{M}_{\gamma}$
        \State \quad predict $s', r \gets T_{\delta}(s,a)$
        \State \quad $\text{mse\_loss} \gets \text{MSE(predictions, actual)}$
        \State \quad $\text{regularization} \gets \text{MSE}(T_{\delta,\texttt{old}}, T_{\delta})$
        \State \quad $\text{loss} \gets \text{mse\_loss} + \beta \cdot \text{regularization}$
        \State \quad Update $H_{\varphi}$ by minimizing $\text{loss}$
        \State Generate one-step rollout using $H_{\varphi}$ and $T_{\delta}$:
        \State \quad sample actual state $s$ from $\mathcal{M}_{\alpha}$
        \State \quad take random action $a$ using $\pi_{\theta}$
        \State \quad use $(s,a)$ to generate synthetic $(s',r)$ using $H_{\varphi}$ and $T_{\delta}$
        \State \quad store tuple $(s,a,s',r)$ in $\mathcal{M}_{\beta}$
        \State Update policy $\pi_{\theta}$ using data from $\mathcal{M}_{\alpha}, \mathcal{M}_{\beta}$
    \EndFor
    \State Optionally store $H_{\varphi}$ state or evaluate across tasks
\EndFor

\end{algorithmic}
\end{algorithm}
