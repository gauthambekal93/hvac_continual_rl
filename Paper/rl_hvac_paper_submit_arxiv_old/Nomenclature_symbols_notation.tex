

\subsection*{Nomenclature}
\begin{description}
  \item[HVAC] Heating, Ventilation, and Air Conditioning
  \item[DRL] Deep Reinforcement Learning
  \item[RL] Reinforcement Learning
  \item[TL] Transfer Learning
  \item[CL] Continual Learning
  \item[MFRL] Model-Free Reinforcement Learning
  \item[MBRL] Model-Based Reinforcement Learning
  \item[SAC] Soft Actor Critic
  \item[MPC] Model Predictive Control
  \item[BOPTEST] Building Optimization Testing Framework
  \item[BHHP] BOPTEST Hydronic Heat Pump
  \item[ML] Machine Learning
  \item[NN] Neural Network
  \item[RL Agent] Reinforcement Learning Agent (Actor-Critic or Q-learning based)
  \item[Hypernet] Hypernetwork (generates weights for a target network)
  \item[Catastrophic Forgetting] Rapid loss of previously learned knowledge when adapting to new tasks
  \item[Dyna] Integrated architecture combining learning, planning, and reacting
\end{description}

\subsection*{Symbols and Notation}
\begin{description}
  \item[\(\gamma\)] Discount factor for future rewards, \(0 < \gamma \le 1\)
  \item[\(\alpha\)] Learning rate or regularization coefficient (context-dependent)
  \item[\(\beta\)] Regularization term coefficient used in hypernetwork training
  \item[\(s\)] Environment state (for example, zone temperature or time)
  \item[\(a\)] Action selected by the RL agent (such as a discretized control input)
  \item[\(r\)] Reward returned by the environment for a given \(s\)-\(a\) pair
  \item[\(s'\)] Next state resulting from the transition \((s,a)\)
  \item[\(\pi_\theta\)] Policy network with trainable parameters \(\theta\)
  \item[\(Q_\phi\)] State-action value (critic) network with parameters \(\phi\)
  \item[\(H_\varphi\)] Hypernetwork with parameters \(\varphi\)
  \item[\(T_\delta\)] Target network (e.g., a dynamics or reward model) generated by \(H_\varphi\), with parameters \(\delta\)
  \item[\(\mathcal{M}_\alpha\)] Real data memory buffer
  \item[\(\mathcal{M}_\beta\)] Synthetic data buffer (model-generated samples)
  \item[\(\mathcal{M}_\gamma\)] Hypernetwork training data buffer
  \item[\(K_i\)] Number of training episodes or iterations for task \(i\)
  \item[\(\text{task-id}\)] One-hot identifier specifying the current task
  \item[\(\text{layer-id}\)] Layer index used by \(H_\varphi\) to generate target network parameters
  \item[\(P(s' \mid s,a)\)] Transition dynamics (approximated by the hypernetwork)
  \item[\(\eta\)] Learning rate for the SAC or hypernetwork optimizer
\end{description}

